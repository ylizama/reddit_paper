{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c4fd59",
   "metadata": {},
   "source": [
    "## 1. Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "65c8176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "import json \n",
    "import csv \n",
    "import datetime\n",
    "import time\n",
    "\n",
    "COMMENTS = 'comment'\n",
    "SUBMISSION = 'submission'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd41aed",
   "metadata": {},
   "source": [
    "## 2. Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a28f2c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def send_query(query, after, before, subreddit, endpoint, size=100):\n",
    "    \n",
    "    \"\"\"Makes the API Request and return a limit of 100 records (maximum allowed)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str\n",
    "        search criteria keywords\n",
    "    after : timestamp\n",
    "        timestamp of the start date of searching interval\n",
    "    before : timestamp\n",
    "        timestamp of the end date of searching interval  \n",
    "    subreddit : str, optional\n",
    "        narrow the result to a specific subreddit. If not specified, it will search in all reddit data.\n",
    "    size: int, default=100\n",
    "        maximum number of results in a query \n",
    "    endpoint: str\n",
    "        API endpoint: comment or submission. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        a list of dicts. Each dict contains information related to a post/comment. \n",
    "    \"\"\"\n",
    "    \n",
    "    # create url with search parameters\n",
    "    url = f\"https://api.pushshift.io/reddit/search/{endpoint}?q={query}&size={size}&after={after}&before={before}&subreddit={subreddit}&sort=asc&metadata=true\"\n",
    "    \n",
    "    r = requests.get(url)      #request data to API\n",
    "    if r.status_code == 429:\n",
    "        time.sleep(5)\n",
    "        r = requests.get(url)\n",
    "\n",
    "    return json.loads(r.text)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0859ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_submissions_features(entity):\n",
    "    \n",
    "    \"\"\"Filter relevant attributes from a submission\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entity : dict\n",
    "        Dict object that contains all information about a submission returned in the API request. \n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    submission_id: str\n",
    "        Id of the submission\n",
    "    records: list\n",
    "        List of values \n",
    "    attributes: list \n",
    "        List of name of the attributes of the submission\n",
    "    \"\"\"\n",
    "    \n",
    "    records = list() \n",
    "    \n",
    "    title = entity['title']\n",
    "    url = entity['url']\n",
    "    \n",
    "    try:\n",
    "        flair = entity['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"    \n",
    "        \n",
    "    author = entity['author']\n",
    "    submission_id = entity['id']\n",
    "    score = entity['score']\n",
    "    created = datetime.datetime.fromtimestamp(entity['created_utc']) \n",
    "    num_comments = entity['num_comments']\n",
    "    permalink = entity['permalink']\n",
    "    selftext= entity['selftext']\n",
    "    subreddit= entity['subreddit']\n",
    "    subreddit_id= entity['subreddit_id']\n",
    "    \n",
    "    records.append((submission_id,title,url,author,score,created,num_comments,permalink,flair, selftext,subreddit,subreddit_id))\n",
    "    \n",
    "    attributes = ['submission_id','title','url','author','score','created','num_comments','permalink','flair', 'selftext','subreddit','subreddit_id']\n",
    "    \n",
    "    return submission_id, records, attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3c5f4b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comment_features(entity):\n",
    "    \n",
    "    \"\"\"Filter relevant attributes from a comment\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entity : dict\n",
    "        Dict object that contains all information about a comment returned in the API request. \n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    comment_id: str\n",
    "        Id of the comment\n",
    "    records: list\n",
    "        List of values \n",
    "    attributes: list \n",
    "        List of name of the attributes of the comment\n",
    "    \"\"\"\n",
    "   \n",
    "    records = list() \n",
    "    \n",
    "    #extracting attributes of interest\n",
    "    try:\n",
    "        flair = entity['author_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"    \n",
    "        \n",
    "    author = entity['author']\n",
    "    comment_id = entity['id']\n",
    "    parent_id = entity['parent_id']\n",
    "    score = entity['score']\n",
    "    created = datetime.datetime.fromtimestamp(entity['created_utc']) \n",
    "    permalink = entity['permalink']\n",
    "    body= entity['body']\n",
    "    subreddit= entity['subreddit']\n",
    "    subreddit_id= entity['subreddit_id']\n",
    "\n",
    "    \n",
    "    records.append((comment_id,author,score,created,permalink,flair,body,parent_id, subreddit,subreddit_id, ))\n",
    "    attributes = ['comment_id','author','score','created','permalink','flair','body','parent_id', 'subreddit','subreddit_id']\n",
    "    \n",
    "    return comment_id, records, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d117751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pushshift_data(endpoint, query, after, before, subreddit):\n",
    "    \n",
    "    \"\"\"Makes several API requests (~100 records each) to get all records during the period.  \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    endpoint: str\n",
    "        API endpoint: comment or submission.\n",
    "    query : str\n",
    "        search criteria keywords\n",
    "    after : timestamp\n",
    "        timestamp of the start date of searching interval\n",
    "    before : timestamp\n",
    "        timestamp of the end date of searching interval  \n",
    "    subreddit : str, optional\n",
    "        narrow the result to a specific subreddit. If not specified, it will search in all reddit data.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    all_records: list\n",
    "        List of all search results. \n",
    "    attributes: list \n",
    "        List of name of the attributes of the object(comment or submission). \n",
    "    \"\"\"\n",
    "        \n",
    "    all_records = {}\n",
    "\n",
    "    data = send_query(query, after, before, subreddit, endpoint)\n",
    "    \n",
    "    # make API requests iteratively to get all data during the period specified. \n",
    "    while len(data) > 0:  \n",
    "        for d in data:\n",
    "            if endpoint == SUBMISSION:\n",
    "                id_, observ, attributes = extract_submissions_features(d) \n",
    "                all_records[id_] = observ\n",
    "                \n",
    "            elif endpoint == COMMENTS:\n",
    "                id_, observ, attributes = extract_comment_features(d)\n",
    "                all_records[id_] = observ\n",
    "\n",
    "        after = data[-1]['created_utc'] #update start date with the date of the latest harvested record \n",
    "        data = send_query(query, after, before, subreddit, endpoint)\n",
    "        \n",
    "    print(str(len(all_records)), endpoint +'s', \"have been downloaded\")\n",
    "    return all_records, attributes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5e176ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_reddit_data(endpoint, query, after, before, subreddit):\n",
    "    \"\"\" Harvest data from Reddit using the Pushshift API. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    endpoint: str\n",
    "        API endpoint: comment or submission.\n",
    "    query : str\n",
    "        search criteria keywords\n",
    "    after : timestamp\n",
    "        timestamp of the start date of searching interval\n",
    "    before : timestamp\n",
    "        timestamp of the end date of searching interval  \n",
    "    subreddit : str, optional\n",
    "        narrow the result to a specific subreddit. If not specified, it will search in all reddit data.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "     csv file containing all query results.  \n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    all_records, attributes = get_pushshift_data(endpoint,query, after, before, subreddit)\n",
    "    \n",
    "    upload_count = 0\n",
    "    file = 'data_'+ endpoint +'.csv' #PLACE THE NAME OF YOUR FILE HERE\n",
    "    \n",
    "    with open(file, 'w', newline='', encoding='utf-8') as file: \n",
    "        a = csv.writer(file, delimiter=',')\n",
    "       \n",
    "        a.writerow(attributes)\n",
    "       \n",
    "        for r in all_records:\n",
    "            a.writerow(all_records[r][0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "87cb6e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1580536800&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1586253793&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1587473586&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1588363068&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1589401594&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1592392571&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1596457477&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1602938585&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1614834907&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1629950156&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "829 comments have been downloaded\n",
      "https://api.pushshift.io/reddit/search/submission?q=covid+lockdown&size=100&after=1580536800&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "<Response [429]>\n",
      "https://api.pushshift.io/reddit/search/submission?q=covid+lockdown&size=100&after=1626145015&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/submission?q=covid+lockdown&size=100&after=1629268725&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "105 submissions have been downloaded\n"
     ]
    }
   ],
   "source": [
    "#search parameters\n",
    "after = \"1580536800\"       #Start timestamp (1580536800 = 01 Feb 2020)\n",
    "before = \"1630126800\"      #End timestamp (1630126800 = 28 Aug 2021)\n",
    "query = \"covid+lockdown\"   #query: posts/comments that included the terms 'covid' and 'lockdown'\n",
    "subreddit = \"covid19\"      #search for posts/comments in the subreddit 'r/covid19'\n",
    "\n",
    "#harvest comments\n",
    "harvest_reddit_data(COMMENTS, query, after, before, subreddit)\n",
    "\n",
    "\n",
    "# harvest submissions\n",
    "harvest_reddit_data(SUBMISSION, query, after, before, subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c19f245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1580536800&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1586253793&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1587473586&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1588363068&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1589401594&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1592392571&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1596457477&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1602938585&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1614834907&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "<html>\n",
      "<head><title>429 Too Many Requests</title></head>\n",
      "<body bgcolor=\"white\">\n",
      "<center><h1>429 Too Many Requests</h1></center>\n",
      "<hr><center>nginx/1.14.0</center>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "https://api.pushshift.io/reddit/search/comment?q=covid+lockdown&size=100&after=1629950156&before=1630126800&subreddit=covid19&sort=asc&metadata=true\n",
      "829 comments have been downloaded\n"
     ]
    }
   ],
   "source": [
    "#harvest comments\n",
    "harvest_reddit_data(COMMENTS, query, after, before, subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ffb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# harvest submissions\n",
    "harvest_reddit_data(SUBMISSION, query, after, before, subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6165d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a03e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
